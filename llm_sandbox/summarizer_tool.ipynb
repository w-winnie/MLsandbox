{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19102427",
   "metadata": {},
   "source": [
    "Notebook containing tools to summarize AND interact with different content types (websites, pdfs, ppts, videos) <br>\n",
    "The Notebook employs various python packages to extract required contents from URLs <br>\n",
    "And employs an llm connector to connect to frontier llms (llama, mixtral, gemini, gpt etc - gpt-4o in this case) <br>\n",
    "There are different sections for eacvh content type and it contains summary and qna - which can be run in Test section under each <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40582dbb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04db731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm_connector import create_model_client, chat_with_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ca48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website Scraper\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa691a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF reader\n",
    "# !pip install PyPDF2\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834b3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPT \n",
    "# !pip install python-pptx\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f7d9b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube\n",
    "# !pip install youtube-transcript-api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb51f5",
   "metadata": {},
   "source": [
    "# Universal Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(model_name, user_message, system_message=None):\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    if system_message:\n",
    "        print(f\"With system prompt: {system_message[:50]}...\")\n",
    "    if user_message:\n",
    "        print(f\"With user prompt: {user_message[:50]}...\")\n",
    "    try:\n",
    "        response = chat_with_model(model_name, user_message, system_message)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = get_model_response(\n",
    "    model_name=\"gpt-4o\",\n",
    "    user_message=\"What are the different types of clouds?\",\n",
    "    system_message=\"You are a helpful assistant with an expertise in meterology\"\n",
    ")\n",
    "\n",
    "if response:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a890db1",
   "metadata": {},
   "source": [
    "# Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40654a8",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b49bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048383f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34152456",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195e30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_website_summary = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b321f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_website_summary(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2d2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_website(url):\n",
    "    website = Website(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_website_summary(website),\n",
    "        system_message=system_prompt_website_summary\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188e07d",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6948b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_website_summary(url):\n",
    "    summary = summarize_website(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca1c6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a website titled Foundation Mod...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The article from the Netflix Technology Blog discusses the new foundation model for personalized recommendations being developed by Netflix's engineering team. As Netflix expands its personalization algorithms, the challenge has been in maintaining these models and transferring innovations across different systems. Inspired by the evolution from multiple small models to large language models in NLP, Netflix aims to create a centralized recommendation model. This model uses data-centric approaches and semi-supervised learning to process billions of user interactions effectively, applying techniques like interaction tokenization and sparse attention to enhance scalability and accuracy. The model also addresses unique challenges in recommendation systems, like entity cold-starting, by integrating metadata as well as user interactions. Its applications span predictive modeling and generating user and entity embeddings that can be used across various services, marking a significant shift from specialized models to a comprehensive system. The blog underscores Netflix's continued innovation in improving user experiences through advanced machine learning and AI techniques."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_website_summary(\"https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a451ac2",
   "metadata": {},
   "source": [
    "## QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0d66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_website_qna = \"You are an assistant that analyzes the contents of a website \\\n",
    "and answers questions related to the content of the website, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a2d78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_website_qna(website, question):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows \\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    user_prompt += f\"\\n\\nPlease answer the following question related to this website: {question}\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f165e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def website_qna(url, question):\n",
    "    website = Website(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_website_qna(website, question),\n",
    "        system_message=system_prompt_website_qna\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742c2f0",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a6164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_website_answer(url, question):\n",
    "    answer = website_qna(url, question)\n",
    "    display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382d210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a website titled Foundation Mod...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The blog post from Netflix TechBlog discusses the development of a Foundation Model for Personalized Recommendations, drawing inspiration from the advancements in Large Language Models (LLMs). Let’s break down the key elements from a data scientist's perspective:\n",
       "\n",
       "### Model and Approach\n",
       "\n",
       "1. **Unified Recommender System**: Netflix is evolving from using multiple specialized machine learning models for different recommendation tasks (e.g., “Continue Watching,” “Today’s Top Picks for You”) to a centralized recommendation architecture. This design aims to improve efficiency, maintainability, and the ability to propagate innovations across different recommendation tasks.\n",
       "\n",
       "2. **Foundation Model Concept**: Inspired by the paradigm shift in NLP towards large language models that can perform multiple tasks, Netflix is implementing a similar approach in recommendations. The foundational recommendation model aggregates and generalizes user interaction data, catering to various downstream recommendation tasks.\n",
       "\n",
       "3. **Data-Centric Approach**: Similar to LLMs, the focus shifts from a model-centric approach (heavily reliant on feature engineering) to a data-centric one. Emphasis is placed on accumulating large-scale, high-quality data and employing end-to-end learning where practical.\n",
       "\n",
       "4. **Tokenization of User Interactions**: User actions are tokenized to create meaningful sequences, preserving essential information like watch duration and engagement types. This process balances data granularity with sequence compression to keep training and inference feasible within real-world constraints.\n",
       "\n",
       "5. **Model Architecture**:\n",
       "   - **Sparse Attention and Sliding Window Sampling**: To handle the extensive interaction histories that surpass the typical context windows of recommendation models, techniques like sparse attention and sliding window sampling are used. These methods help process longer sequences efficiently.\n",
       "\n",
       "6. **Training Objectives**:\n",
       "    - **Autoregressive Next-Token Prediction**: This aligns with the GPT model's training objective and exploits large-scale unlabeled data.\n",
       "    - Adjustments are made to account for the varying importance of different interactions, such as distinguishing between short trailer plays and full movie watches.\n",
       "    - Multi-token prediction adjusts the focus to capture long-term dependencies more effectively than single-token prediction might.\n",
       "\n",
       "7. **Cold Start Solutions**:\n",
       "    - **Incremental Training**: Life-long learning capabilities, such as reusable parameters for new content, enable rapid adaptation as titles are added to the platform.\n",
       "\n",
       "8. **Embeddings and Metadata**:\n",
       "    - New titles incorporate metadata into their embeddings to provide reasonable starting points until user interaction data become available. The combined use of metadata and IDs address cold-start issues.\n",
       "\n",
       "### Relationship to LLMs\n",
       "\n",
       "Netflix applies insights from LLMs in the following ways:\n",
       "\n",
       "- **Transformation from Specialized Models to Foundation Levels**: Following LLMs’ transformation, the idea is to move from discrete, specialized models to a single, versatile model.\n",
       "- **Large-Scale Semi-Supervised Training**: Like LLMs, this model benefits from semi-supervised training on a wide array of unlabeled data to acquire a deep understanding of member preferences.\n",
       "- **End-to-End Learning and Scaling**: The scaling law seen in LLMs for increased performance with larger models and data also applies to Netflix’s foundation model. The aim is to facilitate expansive data engagement and generate better recommendation solutions.\n",
       "\n",
       "Overall, this blog highlights the technical nuances of transitioning to a more holistic, scalable recommendation system, leveraging ideas from LLM successes to enhance personalization capabilities across Netflix’s vast viewership and content catalog."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_website_answer(\"https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39\", \"From a data scientist's perspective, what is this blog sayin? please explain in detail the model they're using, whats the approach, and how is this about LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60a747a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a website titled Foundation Mod...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To fully grasp Netflix's new approach to personalized recommendations, it's worth breaking it down component by component as they've shifted strategies compared to their previous systems:\n",
       "\n",
       "### Previous Approach:\n",
       "Netflix historically used multiple specialized models for different recommendation functions like \"Continue Watching\" or \"Today's Top Picks.\" Each model operated largely independently, presenting challenges in sharing insights or leveraging innovations across models, especially as these were trained separately on similar datasets.\n",
       "\n",
       "### New Approach:\n",
       "\n",
       "#### 1. **Tokenization of User Behavior:**\n",
       "   - **Tokenization** is analogous to how language models process sentences. For user interactions, not every action is of equal relevance (e.g., browsing vs. watching an entire movie). Netflix condenses sequences of interactions (like watching trailers, pausing, or finishing a movie) into more meaningful \"tokens.\" This is similar to NLP's Byte Pair Encoding (BPE). For instance, two consecutive actions like \"played episode\" and \"binge-watched season\" might be merged into a \"completed series\" token, summarizing user behavior compactly.\n",
       "\n",
       "#### 2. **Autoregressive Next-Token Prediction:**\n",
       "   - **Autoregressive modeling** involves predicting the next item in a sequence using prior data. Here, after tokenizing user behaviors, the model predicts what the user will likely interact with next. Compared to language models predicting the next word, Netflix's model uses this method to predict the next viewing choice, considering both immediate past actions and longer-term user patterns.\n",
       "\n",
       "#### 3. **Handling Metadata and Embeddings:**\n",
       "   - Each title or interaction isn't just labeled with a unique ID but also packed with **metadata**, like genre, release date, and language. This metadata gets turned into **embeddings**, which are numerical representations capturing the item's properties. New or unseen content uses these metadata embeddings, allowing recommendations even without previous user interactions. For example, a newly released sci-fi film may derive part of its embedding from similar sci-fi titles' features, helping gauge if it's likely to appeal to the user.\n",
       "\n",
       "### Example:\n",
       "\n",
       "Let's imagine a typical user scenario:\n",
       "\n",
       "- **Old Approach:** If a user watched dark comedies, one model might suggest more comedies based on genre, another might suggest titles starring the same actors. These models might not effectively share insights.\n",
       "  \n",
       "- **New Approach:**\n",
       "  - First, the user's behavior is tokenized—watching the entire \"Black Mirror\" series, pausing to rewatch favorite scenes, and searching for similar dystopian content might become a \"sci-fi dystopia enthusiast\" token.\n",
       "  - The system predicts the user's next interest based on all historical tokens, identifying that they might enjoy a \"new psychological thriller.\"\n",
       "  - A new movie with no past user interactions leverages metadata—its genre, reminiscent of \"Black Mirror,\" boosts its recommendation potential via initial metadata-based embeddings.\n",
       "\n",
       "Overall, Netflix's foundation model integrates various interactive actions and metadata into a comprehensive system, leveraging insights from large language models to improve recommendation accuracy and adaptability. By focusing on a unified, data-centric strategy, Netflix can effectively manage extensive user data and adapt to new titles and changing user preferences more efficiently."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_website_answer(\"https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39\", \"I don't understand, this is so high level -what is the difference from their previous approach, how can you tokenize user behaviour, what do you mean by Autoregressive Next-Token Prediction, how are metadata and embeddings handled? i'm a data scientist, but i still dont see a clear picture of what they're doing, explain clearly with an example please?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358acc1a",
   "metadata": {},
   "source": [
    "# Research Paper\n",
    "passes the whole paper (better suited for smaller reserach papers/articles) <br>\n",
    "so for long papers refer to other solution in PDF_LLM repo with vector db and embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a73b0",
   "metadata": {},
   "source": [
    "## Pdf Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a198be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        parsed_url = urlparse(url)\n",
    "        self.text = \"\"\n",
    "        self.title = \"No title found\"\n",
    "\n",
    "        if parsed_url.scheme in ('http', 'https'):\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                pdf_bytes = BytesIO(response.content)\n",
    "                reader = PdfReader(pdf_bytes)\n",
    "                self._extract(reader)\n",
    "            else:\n",
    "                print(f\"Failed to fetch PDF. Status code: {response.status_code}\")\n",
    "        elif os.path.isfile(url):\n",
    "            try:\n",
    "                with open(url, 'rb') as pdf_file:\n",
    "                    reader = PdfReader(pdf_file)\n",
    "                    self._extract(reader)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading PDF: {e}\")\n",
    "        else:\n",
    "            print(f\"Invalid file path or URL: {url}\")\n",
    "\n",
    "    def _extract(self, reader):\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "        self.text = text\n",
    "        try:\n",
    "            self.title = reader.metadata.get(\"/Title\", \"No title found\") or \"No title found\"\n",
    "        except Exception:\n",
    "            self.title = \"No title found\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Paper(title='{self.title[:1000]}...')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18b580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.000000-29802322') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper(title='Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations...')\n"
     ]
    }
   ],
   "source": [
    "paper = Paper(\"d:/Projects/sandbox/data/ActionsSpeakLouderthanWords.pdf\")\n",
    "print(paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783d3b5",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31437e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_paper_summary = \"You are a research assistant. Your job is to go through the provided article in details, understand the purpose, study the methods and techniques employed, arguments made and the conclusion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35bcb857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_paper_summary(paper):\n",
    "    user_prompt = f\"You are looking at a paper titled {paper.title}\"\n",
    "    user_prompt += \"\\nPlease provide a short summary of this paper in markdown. \\\n",
    "    After the summary extract the following: \\\n",
    "    1) Title and Author of the research paper. \\\n",
    "    2) Year it was published it \\\n",
    "    3) Objective or aim of the research to specify why the research was conducted \\\n",
    "    4) Background or Introduction to explain the need to conduct this research or any topics the readers must have knowledge about \\\n",
    "    5) Type of research/study/experiment to explain what kind of research it is. \\\n",
    "    6) Methods or methodology to explain what the researchers did to conduct the research \\\n",
    "    7) Results and key findings to explain what the researchers found \\\n",
    "    8) Conclusion tells about the conclusions that can be drawn from this research including limitations and future direction \\\n",
    "    The contents of this paper is as follows:\\n\\n\"\n",
    "    user_prompt += paper.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f22c74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(paper_url):\n",
    "    paper = Paper(paper_url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_paper_summary(paper),\n",
    "        system_message=system_prompt_paper_summary\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c6e5b",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b372daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_paper_summary(paper_url):\n",
    "    summary = summarize_paper(paper_url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dac43fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.000000-29802322') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are a research assistant. Your job is to go th...\n",
      "With user prompt: You are looking at a paper titled Actions Speak Lo...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Summary of \"Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\"\n",
       "\n",
       "The paper presents a novel approach to recommendation systems, introducing Generative Recommenders (GRs), a new paradigm that formulates recommendation tasks as sequential transduction tasks. By employing a novel architecture called Hierarchical Sequential Transduction Unit (HSTU), GRs address scaling challenges in recommendation systems, allowing for efficient handling of high cardinality, non-stationary streaming data. HSTU outperforms existing state-of-the-art models both in speed and accuracy. The study demonstrates that HSTU-based GRs can be scaled up to 1.5 trillion parameters, significantly enhancing online performance metrics and enabling a simplification in feature processes for large-scale applications.\n",
       "\n",
       "## Extracted Information\n",
       "\n",
       "1. **Title and Authors of the Paper**\n",
       "   - Title: \"Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations\"\n",
       "   - Authors: Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi\n",
       "\n",
       "2. **Year of Publication**\n",
       "   - 2024\n",
       "\n",
       "3. **Objective/Aim of the Research**\n",
       "   - The research was conducted to explore the use of Transformers in improving the scalability and efficiency of recommendation systems by reformulating traditional Deep Learning Recommendation Models (DLRMs) as sequential transduction tasks within a generative modeling framework.\n",
       "\n",
       "4. **Background/Introduction**\n",
       "   - Recommendation systems are critical for personalizing user experiences on online platforms. Despite their use of vast amounts of data, traditional DLRMs do not scale effectively with computational resources. Inspired by the success of Transformers in language and vision tasks, the authors propose using generative models for recommendations to address the scale, feature heterogeneity, and computational costs in these systems.\n",
       "\n",
       "5. **Type of Research**\n",
       "   - Methodological and applied research focused on designing, implementing, and evaluating a new model architecture for recommendation systems.\n",
       "\n",
       "6. **Methods/Methodology**\n",
       "   - The researchers introduced HSTU, a novel architecture that reformulates recommendation tasks as sequential transduction. The architecture builds on a pooling mechanism that combines categorical and numerical features into a unified time series and manages large vocabulary sizes efficiently. It leverages methods like M-FALCON to reduce inference costs and stochastic length mechanisms to manage sequence sparsity.\n",
       "\n",
       "7. **Results and Key Findings**\n",
       "   - HSTU outperformed existing models like transformers in both speed and accuracy on large sequence data. It significantly improved metrics in online A/B tests by 12.4% when deployed on platforms with billions of users. The study demonstrates that the architectural and methodological improvements allow GRs to scale linearly with training compute across extensive ranges, validated on synthetic and public datasets.\n",
       "\n",
       "8. **Conclusion**\n",
       "   - The paper concludes that by treating user actions as a new modality and using sequentially trained transducers, recommendation systems can achieve considerable efficiency gains and accuracy. The work marks a step toward creating foundation models in recommendation systems akin to LLMs in NLP, which could reduce feature engineering complexity and carbon footprints in model training. Future directions include further refining the generative approach and understanding its broader applications in search and advertisements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_paper_summary(\"d:/Projects/sandbox/data/ActionsSpeakLouderthanWords.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca2324",
   "metadata": {},
   "source": [
    "## QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb31cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_paper_qna = \"You are a research assistant. Your job is to go through the provided article in details, understand the purpose, study the methods and techniques employed, arguments made and the conclusion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "febfad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_paper_qna(website, question):\n",
    "    user_prompt = f\"You are looking at a paper titled {paper.title}. The contents of this paper is as follows:\\n\\n\"\n",
    "    user_prompt += paper.text\n",
    "    user_prompt += f\"\\n\\nPlease answer the following question related to this paper: {question}\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1509a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_qna(url, question):\n",
    "    paper = Paper(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_paper_qna(paper, question),\n",
    "        system_message=system_prompt_paper_qna\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511c11e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e65af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_paper_answer(url, question):\n",
    "    answer = paper_qna(url, question)\n",
    "    display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88856f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.000000-29802322') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are a research assistant. Your job is to go th...\n",
      "With user prompt: You are looking at a paper titled Actions Speak Lo...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "HSTU (Hierarchical Sequential Transduction Unit) is a novel self-attention encoder architecture designed to handle large-scale, high cardinality, non-stationary recommendation data. The key idea behind HSTU is to provide efficient and scalable training and inference for Generative Recommenders through a specialized attention mechanism and architectural design.\n",
       "\n",
       "### How HSTU Works:\n",
       "1. **Architecture Design**: HSTU modifies the traditional attention mechanism by introducing pointwise aggregated attention instead of the standard softmax attention. This change helps in capturing both the intensity of user engagements and the relative ordering of items, which is essential for recommendation tasks.\n",
       "\n",
       "2. **Attention Mechanism**: The pointwise aggregated attention keeps track of the number of prior data points, which is crucial for understanding user preferences, whereas the softmax normalization often loses this context.\n",
       "\n",
       "3. **Efficiency**: HSTU is designed to be more computationally efficient than standard Transformers, achieving significant speed-ups (5.3x to 15.2x faster for certain sequence lengths). It does so using efficient GPU-based computations, leveraging sparse grouped GEMMs to handle large and sparse input sequences effectively.\n",
       "\n",
       "4. **Training**: HSTU uses a new training paradigm called \"generative training,\" which reduces the computational complexity traditionally associated with self-attention mechanisms. This is achieved by sampling rates that allow the model to be trained with less compute while still being exposed to the same amount of information.\n",
       "\n",
       "### Differences from Traditional Recommendation Systems:\n",
       "- **Sequential Transduction**: Traditional recommender systems often rely on static, handcrafted features and are not built to handle the dynamic nature of user interaction data efficiently. HSTU-based systems treat recommendation tasks as sequential transduction tasks, transforming them into a generative modeling problem.\n",
       "  \n",
       "- **Generative Approach**: Instead of the discriminative setting of typical recommenders, which learn to predict user actions or item ranks, this approach models the joint distribution of all user contents and actions sequentially, allowing for richer interaction modeling.\n",
       "\n",
       "- **Unified Feature Space**: HSTU consolidates and encodes both categorical (sparse) and numerical (dense) features into a unified sequence, thereby replacing the heterogeneous feature approach in traditional models with a more integrated method.\n",
       "\n",
       "### Features and Training Data:\n",
       "- **Feature Consolidation**: HSTU merges various categorical features into a sequential format, compressing features that change infrequently and integrating them into the main sequence of user interactions. This is particularly important for capturing long-term user behavior effectively.\n",
       "\n",
       "- **Numerical Features Handling**: While numerical features frequently change and may not be entirely sequentialized due to computational constraints, HSTU aims to capture their essence by removing them and relying on a more expressive architectural design.\n",
       "\n",
       "### Algorithmic Idea:\n",
       "The core algorithmic innovation in HSTU lies in its attention mechanism and its ability to perform computations efficiently by leveraging sparsity and implementing efficient parallel operations. Additionally, M-FALCON (Microbatched-Fast Attention Leveraging Cacheable OperatioNs) is an inference algorithm introduced to efficiently handle large numbers of candidates during the ranking process, further enhancing scalability.\n",
       "\n",
       "Overall, by reformulating recommendations as a generative task and improving efficiency through architectural and algorithmic innovations, HSTU represents a step forward in building recommendation systems that can scale effectively to meet the demands of modern online platforms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_paper_answer(\"d:/Projects/sandbox/data/ActionsSpeakLouderthanWords.pdf\", \"This is very new to me - what are hstu? how do they work? how is this different from traditional recommendation systems? what are the features or training data - whats been done to it? what is the algorithmic idea of this process?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f89e38",
   "metadata": {},
   "source": [
    "# Presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a074cbc",
   "metadata": {},
   "source": [
    "## PPT Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a571854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerPoint:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.slides = []\n",
    "        self.text = \"\"\n",
    "        self.title = \"No title found\"\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            try:\n",
    "                presentation = Presentation(path)\n",
    "                self._extract(presentation)\n",
    "                self.title = presentation.core_properties.title or self.slides[0] if self.slides else \"No title found\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading presentation: {e}\")\n",
    "        else:\n",
    "            print(f\"Invalid file path: {path}\")\n",
    "\n",
    "    def _extract(self, presentation):\n",
    "        all_text = []\n",
    "        for slide in presentation.slides:\n",
    "            slide_text = []\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\") and shape.text.strip():\n",
    "                    slide_text.append(shape.text.strip())\n",
    "            combined_slide = \"\\n\".join(slide_text)\n",
    "            self.slides.append(combined_slide)\n",
    "            all_text.append(combined_slide)\n",
    "        self.text = \"\\n\\n\".join(all_text)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"PresentationFile(title='{self.title[:100]}...', slides={len(self.slides)}, text='{self.text[:1000]}...')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "95be6a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PresentationFile(title='PowerPoint Presentation...', slides=10, text='Wildfire Prediction\n",
      "Proposal\n",
      "\n",
      "Use cases\n",
      "Fire occurrence prediction\n",
      "Area burned and damage prediction\n",
      "Effect of weather\n",
      "Severe weather - Lightning\n",
      "Temperature rise\n",
      "Effect of human proceses\n",
      "\n",
      "Market\n",
      "**To add spending and loss in and due to wildfires in Canada, US and in general\n",
      "**Existing solutions and limitations\n",
      "\n",
      "Data\n",
      "Climate data \n",
      "Wildfire data\n",
      "**Vegetation Data\n",
      "**Human activity\n",
      "\n",
      "Model\n",
      "Proposed - ANN (1)\n",
      "\n",
      "Model II\n",
      "Comparison of models for australia - ann still the better choice because of short term memory (5)\n",
      "\n",
      "Impact\n",
      "Weather Network - goes with the brand image\n",
      "Trust - national provider \n",
      "Disaster management and urban and commercial planning\n",
      "Revenue streams\n",
      "Government - wildfire / Forest department\n",
      "International UN disaster management\n",
      "Tracking of carbon footprint etc. for environmental bodies like Environment Canada, UNEP\n",
      "Private settlements\n",
      "Wood/paper industries or suppliers\n",
      "Visibility and smoke level predictions for cities, air traffic, hospitals\n",
      "\n",
      "Cost\n",
      "** Cost of POC\n",
      "** Cost of data -...')\n"
     ]
    }
   ],
   "source": [
    "test_presentation_path = \"d:/Projects/sandbox/data/WildfirePrediction-Proposal-Draft.pptx\"\n",
    "presentation = PowerPoint(test_presentation_path)\n",
    "print(presentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee898425",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "987e069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_presentation_summary = \"You are an assistant that analyzes the contents of a presentation \\\n",
    "and provides a short summary in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8de1c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_presentation_summary(presentation):\n",
    "    user_prompt = f\"You are looking at a presentation titled {presentation.title}\"\n",
    "    user_prompt += \"\\nThe contents of this presentation is as follows; \\\n",
    "please provide a short summary of this presentation in markdown.\\n\\n\"\n",
    "    user_prompt += presentation.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2d1a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_presentation(url):\n",
    "    presentation = PowerPoint(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_presentation_summary(presentation),\n",
    "        system_message=system_prompt_presentation_summary\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307249b",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bc04dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_presentation_summary(url):\n",
    "    summary = summarize_presentation(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f457635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a presentation titled PowerPoin...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```markdown\n",
       "# Wildfire Prediction Proposal\n",
       "\n",
       "### Overview\n",
       "This presentation outlines a proposal for predicting wildfires, focusing on models that predict fire occurrence, the area burned, and the effects of weather and human processes. The proposal highlights advancements in the use of Artificial Neural Networks (ANNs) for accurate predictions and explores their potential impact on disaster management.\n",
       "\n",
       "### Use Cases\n",
       "- **Fire Occurrence Prediction:** Forecasting the likelihood of wildfire occurrences.\n",
       "- **Damage Prediction:** Estimating the potential area that could be burned.\n",
       "- **Weather Impact:** Understanding the role of severe weather, such as lightning and temperature rise.\n",
       "- **Human Processes:** Analyzing human activities that influence wildfires.\n",
       "\n",
       "### Market\n",
       "- **Financial Impact:** Pending data on wildfire-related spending and losses in Canada and the US.\n",
       "- **Existing Solutions:** Discussion on current solutions and their limitations.\n",
       "\n",
       "### Data Sources\n",
       "- Climate data\n",
       "- Wildfire data\n",
       "- Proposed inclusion of vegetation data and human activity data.\n",
       "\n",
       "### Model Proposal\n",
       "- **ANN Model I:** Proposed model leveraging ANNs.\n",
       "- **Model Comparison:** Comparison with models used in Australia showing the advantages of ANNs due to their effective short-term memory capabilities.\n",
       "\n",
       "### Impact & Benefits\n",
       "- Boosts brand image for weather networks.\n",
       "- Enhances trust as a national provider in disaster management.\n",
       "- Influences urban and commercial planning, benefiting government agencies, international bodies, and industries.\n",
       "- Potential revenue streams include government departments, international disaster management organizations, and private sectors such as wood and paper industries.\n",
       "- Improves visibility and smoke level predictions for public safety and industry operations.\n",
       "\n",
       "### Costs\n",
       "- Estimated cost for proof of concept (POC).\n",
       "- Data acquisition and storage expenses.\n",
       "- Training costs associated with model development.\n",
       "\n",
       "### Implementation\n",
       "Outlined steps for taking the proposed model from concept to application.\n",
       "\n",
       "### References / Resources\n",
       "- Review of ML applications in wildfire sciences - NRC (2020) [Read more](https://cdnsciencepub.com/doi/pdf/10.1139/er-2020-0019#refg23)\n",
       "- Wildland Fire Suppression and Partnerships by the DOI [Learn more](https://www.doi.gov/wildlandfire/suppression)\n",
       "- Forest fire risk prediction system, DOI: 10.1016/S0957-4174(03)00095-2\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_presentation_summary(\"d:/Projects/sandbox/data/WildfirePrediction-Proposal-Draft.pptx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebb902",
   "metadata": {},
   "source": [
    "## QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8bdc3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_presentation_qna = \"You are an assistant that analyzes the contents of a presentation \\\n",
    "and answers questions related to the content of the presentation, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "88769e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_presentation_qna(presentation, question):\n",
    "    user_prompt = f\"You are looking at a presentation titled {presentation.title}\"\n",
    "    user_prompt += \"\\nThe contents of this presentation is as follows \\n\\n\"\n",
    "    user_prompt += presentation.text\n",
    "    user_prompt += f\"\\n\\nPlease answer the following question related to this presentation: {question}\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b20e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def presentation_qna(url, question):\n",
    "    presentation = PowerPoint(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_website_qna(presentation, question),\n",
    "        system_message=system_prompt_website_qna\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1ffc7",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c0bd5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_presentation_answer(url, question):\n",
    "    answer = presentation_qna(url, question)\n",
    "    display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53cd360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a website titled PowerPoint Pre...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The proposed model for wildfire prediction on the website is an Artificial Neural Network (ANN). The use cases outlined include:\n",
       "\n",
       "- Fire occurrence prediction\n",
       "- Area burned and damage prediction\n",
       "- Effect of weather, specifically severe weather like lightning and temperature rise\n",
       "- Impact of human processes on wildfire occurrence and behavior"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_presentation_answer(\"d:/Projects/sandbox/data/WildfirePrediction-Proposal-Draft.pptx\", \"What is the model and use case proposed?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95c4cc",
   "metadata": {},
   "source": [
    "# Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e25a6",
   "metadata": {},
   "source": [
    "## Youtube Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5b6afd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeVideo:\n",
    "    def __init__(self, url, language='en'):\n",
    "        \"\"\"\n",
    "        Initialize the YoutubeVideo object with the video URL and fetch its transcript.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.video_id = self._extract_video_id(url)\n",
    "        self.language = language\n",
    "        self.text = self._get_transcript()\n",
    "        self.title = f\"Video with ID {self.video_id}\"  \n",
    "\n",
    "    def _extract_video_id(self, url):\n",
    "        \"\"\"\n",
    "        Extract the video ID from the given YouTube URL.\n",
    "        \"\"\"\n",
    "        regex = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|\\S*\\?v=)|(?:youtu\\.be\\/))([a-zA-Z0-9_-]{11})\"\n",
    "        match = re.match(regex, url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid YouTube URL\")\n",
    "\n",
    "    def _get_transcript(self):\n",
    "        \"\"\"\n",
    "        Fetch the transcript of the video using the YouTubeTranscriptApi.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = YouTubeTranscriptApi.get_transcript(self.video_id, languages=[self.language])\n",
    "            return \" \".join([item['text'] for item in text])\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        String representation of the YoutubeVideo object.\n",
    "        \"\"\"\n",
    "        return f\"YoutubeVideo(title='{self.title}', video_id='{self.video_id}', transcript='{self.text[:100]}...')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88559f53",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7ff6c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_video_summary = \"You are an assistant that analyzes the contents of a video \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8b888929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_video_summary(video):\n",
    "    user_prompt = f\"You are looking at a video titled {video.title}\"\n",
    "    user_prompt += \"\\nThe contents of this video are transcribed as follows; \\\n",
    "please provide a short summary of this video in markdown. \\n\\n\"\n",
    "    user_prompt += video.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a22e907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_video(url):\n",
    "    video = YoutubeVideo(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_video_summary(video),\n",
    "        system_message=system_prompt_video_summary\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f3425",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a92891d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_summary(url):\n",
    "    summary = summarize_video(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f4c8e097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a video titled Video with ID 8H...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This video explores homeopathy, a widely debated alternative medicine. It explains key principles, such as \"like cures like,\" where substances causing symptoms are used to treat them, and \"potentization,\" involving extreme dilution to enhance effectiveness. Criticisms arise due to the lack of scientific evidence supporting homeopathy's efficacy beyond placebo effects. Despite this, homeopathy remains popular, partly due to its personalized care and emphasis on empathy—an area modern medicine could potentially learn from. The video also discusses the financial scale of the homeopathy industry and its impact on public health. The video is produced by Kurzgesagt, announcing their relaunch of a German channel with new content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_video_summary(\"https://www.youtube.com/watch?v=8HslUzw35mc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df54c1",
   "metadata": {},
   "source": [
    "## QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6c7c7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_video_qna = \"You are an assistant that analyzes the contents of a video \\\n",
    "and answers questions related to the content of the video transcription \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d8bbe748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_video_qna(video, question):\n",
    "    user_prompt = f\"You are looking at a video titled {video.title}\"\n",
    "    user_prompt += \"\\nThe contents of this video is as follows \\n\\n\"\n",
    "    user_prompt += video.text\n",
    "    user_prompt += f\"\\n\\nPlease answer the following question related to this video: {question}\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "75a89412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_qna(url, question):\n",
    "    video = YoutubeVideo(url)\n",
    "    response = get_model_response(\n",
    "        model_name=\"gpt-4o\",\n",
    "        user_message=get_user_prompt_video_qna(video, question),\n",
    "        system_message=system_prompt_video_qna\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8ed29",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "feb30545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_answer(url, question):\n",
    "    answer = video_qna(url, question)\n",
    "    display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4o\n",
      "With system prompt: You are an assistant that analyzes the contents of...\n",
      "With user prompt: You are looking at a video titled Video with ID 8H...\n",
      "Using OpenAI with model: gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The video mentions that the global market for homeopathy is expected to reach over $17 billion by 2024. This suggests that homeopathy is a significant industry with substantial financial influence, comparable to other sectors within the pharmaceutical landscape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_video_answer(\"https://www.youtube.com/watch?v=8HslUzw35mc\", \"What is the financial share of homeopathy?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
